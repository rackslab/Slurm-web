#!/usr/bin/env python3
#
# Copyright (c) 2024 Rackslab
#
# This file is part of Slurm-web.
#
# SPDX-License-Identifier: GPL-3.0-or-later

from pathlib import Path
import sys
import getpass
import random
import logging
import time
from datetime import datetime, timedelta

from rfl.log import setup_logger

from crawler.lib import (
    load_settings,
    DevelopmentHostClient,
    DevelopmentHostConnectionError,
    DevelopmentHostCluster,
)
from crawler.slurmrestd import SlurmrestdCrawler
from crawler.agent import crawl_agent
from crawler.gateway import slurmweb_token, GatewayCrawler
from crawler.prometheus import crawl_prometheus

from racksdb import RacksDB
from slurmweb.slurmrestd.auth import SlurmrestdAuthentifier

logger = logging.getLogger("crawl-tests-assets")

DEBUG_FLAGS = ["slurmweb", "rfl", "werkzeug", "urllib3", "crawler"]
DEV_HOST = "firehpc.dev.rackslab.io"
USER = getpass.getuser()
GATEWAY_PREFERRED_INFRASTRUCTURE = "tiny"
METRICS_PREFERRED_INFRASTRUCTURE = "emulator"
# Map between infrastructure names and cluster names that are visible in Slurm-web.
MAP_CLUSTER_NAMES = {"emulator": "atlas"}


def slurmweb_cluster_name(infrastructure: str):
    return MAP_CLUSTER_NAMES.get(infrastructure, infrastructure)


def main() -> None:
    """Crawl and save test assets from Slurm-web gateway, agent and slurmrestd."""

    # FIXME: support possibility to restrict cluster

    # Setup logger
    setup_logger(
        debug=True,
        log_flags=["ALL"],
        debug_flags=DEBUG_FLAGS,
    )

    # Search for slurm-web development environment temporary directory
    dev_tmp_dirs = list(Path("/tmp").glob("slurm-web-*"))
    try:
        assert len(dev_tmp_dirs) == 1
    except AssertionError:
        logger.error(
            "Unexpectedly found %d Slurm-web development temporary directories",
            len(dev_tmp_dirs),
        )
        sys.exit(1)
    dev_tmp_dir = dev_tmp_dirs[0]
    logger.info(
        "Slurm-web development environment temporary directory: %s", dev_tmp_dir
    )

    # Load cluster list from RacksDB database
    db = RacksDB.load(db="dev/firehpc/db", schema="../RacksDB/schemas/racksdb.yml")
    logger.info("List of clusters: %s", db.infrastructures.keys())

    dev_host = DevelopmentHostClient(DEV_HOST, USER)
    try:
        dev_host.connect()
    except DevelopmentHostConnectionError as err:
        logger.error(err)
        sys.exit(1)

    # Get Slurm-web JWT for authentication on gateway and agent
    token = slurmweb_token(
        dev_host,
        slurmweb_cluster_name(GATEWAY_PREFERRED_INFRASTRUCTURE),
        GATEWAY_PREFERRED_INFRASTRUCTURE,
        dev_tmp_dir,
    )

    for infrastructure in db.infrastructures.keys():
        # Load agent configuration
        settings = load_settings(
            "conf/vendor/agent.yml", dev_tmp_dir, f"agent-{infrastructure}.ini"
        )
        auth = SlurmrestdAuthentifier(
            settings.slurmrestd.auth,
            settings.slurmrestd.jwt_mode,
            settings.slurmrestd.jwt_user,
            settings.slurmrestd.jwt_key,
            settings.slurmrestd.jwt_lifespan,
            settings.slurmrestd.jwt_token,
        )
        cluster = DevelopmentHostCluster(
            dev_host, infrastructure, settings.slurmrestd.uri, auth
        )

        # Cancel all jobs
        cluster.cancel_all()

        # Resume all nodes
        cluster.nodes_resume()

        # FIXME: check all nodes are OK

        # Move some nodes from production
        nodes = cluster.nodes_partition(cluster.partitions()[0])
        node_down = random.choice(nodes)
        node_drain = random.choice([node for node in nodes if node != node_down])
        cluster.node_update(node_down, "DOWN", "CPU dead")
        cluster.node_update(node_drain, "DRAIN", "ECC memory error")

        # Create reservation
        account = cluster.pick_account()
        cluster.reservation(
            "training",
            cluster.partitions()[0],
            accounts=[account],
            users=cluster.pick_account_users(account, 2),
            start=datetime.now() + timedelta(days=5),
            end=datetime.now() + timedelta(days=6),
        )

        # Check has no jobs in queue or skip it
        # FIXME: wait cluster to become idle
        if cluster.has_jobs():
            logger.error(
                "Unable to crawl cluster %s with jobs in queue", infrastructure
            )
            continue
        # Submit timeout job
        # FIXME
        cluster.submit(
            cluster.pick_user(),
            [
                "--ntasks",
                str(1),
            ],
            duration=90,
            timelimit=1,
            wait_running=False,
        )
        # Submit completed job
        cluster.submit(
            cluster.pick_user(),
            [
                "--ntasks",
                str(1),
            ],
            duration=1,
            wait_running=False,
        )
        # Submit failed job
        cluster.submit(
            cluster.pick_user(),
            [
                "--ntasks",
                str(1),
            ],
            duration=1,
            wait_running=False,
            success=False,
        )
        # Submit 10 random running jobs
        for idx in range(10):
            cluster.submit(
                cluster.pick_user(),
                [
                    "--ntasks",
                    str(random.choice([1, 4, 16, 32, 64, 128])),
                ],
                duration=random.choice([30, 60, 90]),
                timelimit=2,
                wait_running=False,
            )
        # Submit pending job
        cluster.submit(
            cluster.pick_user(),
            [
                "--ntasks",
                str(random.choice([1, 4, 16, 32, 64, 128])),
                "--begin",
                "now+1hour",
            ],
            duration=30,
            wait_running=False,
        )
        # Wait for job to timeout
        logging.info("Waiting for job to timeoutâ€¦")
        time.sleep(70)

        # Crawl assets from components
        if infrastructure == GATEWAY_PREFERRED_INFRASTRUCTURE:
            gateway = GatewayCrawler(
                token,
                slurmweb_cluster_name(infrastructure),
                infrastructure,
                dev_tmp_dir,
            )
            gateway.crawl(
                [
                    "clusters",
                    "users",
                    "login",
                    "stats",
                    "jobs",
                    "nodes",
                    "partitions",
                    "qos",
                    "reservations",
                    "accounts",
                    "racksdb",
                    "metrics",
                ]
            )
        crawl_metrics = infrastructure == METRICS_PREFERRED_INFRASTRUCTURE

        # Crawl agent
        crawl_agent(settings.service.port, token, metrics=crawl_metrics)

        # Crawl prometheus
        if crawl_metrics:
            crawl_prometheus(settings.metrics.host.geturl(), settings.metrics.job)

        slurmrestd = SlurmrestdCrawler(cluster, auth)
        slurmrestd.crawl(
            [
                "ping",
                "errors",
                "jobs",
                "nodes",
                "partitions",
                "qos",
                "accounts",
                "reservations",
            ]
        )

        # Resume node states
        cluster.node_resume(node_down)
        cluster.node_resume(node_drain)

        # Delete reservation
        cluster.reservation_delete("training")

        # If cluster has gpu, launch individual jobs
        nodes = cluster.query_slurmrestd_json(f"/slurm/v{cluster.api}/nodes")

        # Look at GPU declared on cluster. Logic expects all nodes with GPUs share the
        # same GPUs setup.
        has_gpu = False
        gpu_per_node = 0
        gpu_types = []
        gpu_partition = None

        for node in nodes["nodes"]:
            all_gres = node["gres"].split(",")
            if len(all_gres) and any([gres_s.startswith("gpu") for gres_s in all_gres]):
                has_gpu = True
                for gres_s in all_gres:
                    gres = gres_s.split(":")
                    # skip non-gpu gres
                    if gres[0] != "gpu":
                        continue
                    gpu_types.append(gres[1])
                    gpu_per_node += int(gres[2])
                    gpu_partition = node["partitions"][0]
                break

        # Submit jobs and allocate GPUs
        if has_gpu:
            # Pick random user on cluster to submit jobs
            user = cluster.pick_user()

            # sbatch --gpus <n> mono-node running
            job_id = cluster.submit(
                user, ["--partition", gpu_partition, "--gpus", str(gpu_per_node)]
            )
            slurmrestd.crawl(["job-gpus-running"], job_id)
            if infrastructure == GATEWAY_PREFERRED_INFRASTRUCTURE:
                gateway.crawl(["job-gpus-running"], job_id)
            cluster.cancel(user, job_id)

            # sbatch --gpus <n> mono-node pending
            job_id = cluster.submit(
                user,
                [
                    "--partition",
                    gpu_partition,
                    "--gpus",
                    str(gpu_per_node),
                    "--begin",
                    "now+1hour",
                ],
                wait_running=False,
            )
            slurmrestd.crawl(["job-gpus-pending"], job_id)
            if infrastructure == GATEWAY_PREFERRED_INFRASTRUCTURE:
                gateway.crawl(["job-gpus-pending"], job_id)
            cluster.cancel(user, job_id)

            # sbatch --gpus <n> with short runtime and wait it to be completed
            # and then removed from slurmctld queue (ie. archived)
            job_id = cluster.submit(
                user,
                ["--partition", gpu_partition, "--gpus", str(gpu_per_node)],
                duration=30,
            )
            time.sleep(30)
            slurmrestd.crawl(["job-gpus-completed"], job_id)
            if infrastructure == GATEWAY_PREFERRED_INFRASTRUCTURE:
                gateway.crawl(["job-gpus-completed"], job_id)
            time.sleep(600)
            slurmrestd.crawl(["job-gpus-archived"], job_id)
            if infrastructure == GATEWAY_PREFERRED_INFRASTRUCTURE:
                gateway.crawl(["job-gpus-archived"], job_id)

            # sbatch --gpus <n> multi-nodes
            job_id = cluster.submit(
                user,
                ["--partition", gpu_partition, "--gpus", str(gpu_per_node * 2)],
            )
            slurmrestd.crawl(["job-gpus-multi-nodes"], job_id)
            if infrastructure == GATEWAY_PREFERRED_INFRASTRUCTURE:
                gateway.crawl(["job-gpus-multi-nodes"], job_id)
            cluster.cancel(user, job_id)

            # sbatch --gpus type:<n>
            job_id = cluster.submit(
                user, ["--partition", gpu_partition, "--gpus", f"{gpu_types[0]}:1"]
            )
            slurmrestd.crawl(["job-gpus-type"], job_id)
            if infrastructure == GATEWAY_PREFERRED_INFRASTRUCTURE:
                gateway.crawl(["job-gpus-type"], job_id)
            cluster.cancel(user, job_id)

            # sbatch --gpus-per-node <m> --nodes <n>
            job_id = cluster.submit(
                user,
                [
                    "--partition",
                    gpu_partition,
                    "--gpus-per-node",
                    str(1),
                    "--nodes",
                    str(2),
                ],
            )
            slurmrestd.crawl(["job-gpus-per-node"], job_id)
            if infrastructure == GATEWAY_PREFERRED_INFRASTRUCTURE:
                gateway.crawl(["job-gpus-per-node"], job_id)
            cluster.cancel(user, job_id)

            # sbatch --gpus-per-node type1:<n>,type2:<m>
            job_id = cluster.submit(
                user,
                [
                    "--partition",
                    gpu_partition,
                    "--gpus-per-node",
                    ",".join([f"{gpu_type}:1" for gpu_type in gpu_types]),
                ],
            )
            slurmrestd.crawl(["job-gpus-multi-types"], job_id)
            if infrastructure == GATEWAY_PREFERRED_INFRASTRUCTURE:
                gateway.crawl(["job-gpus-multi-types"], job_id)

            cluster.cancel(user, job_id)

            # sbatch --gpus-per-socket --sockets-per-node
            job_id = cluster.submit(
                user,
                [
                    "--partition",
                    gpu_partition,
                    "--gpus-per-socket",
                    str(2),
                    "--sockets-per-node",
                    str(2),
                ],
            )
            slurmrestd.crawl(["job-gpus-per-socket"], job_id)
            if infrastructure == GATEWAY_PREFERRED_INFRASTRUCTURE:
                gateway.crawl(["job-gpus-per-socket"], job_id)
            cluster.cancel(user, job_id)

            # sbatch --gpus-per-task <m> --ntasks <n>
            job_id = cluster.submit(
                user,
                [
                    "--partition",
                    gpu_partition,
                    "--gpus-per-task",
                    str(2),
                    "--ntasks",
                    str(2),
                ],
            )
            slurmrestd.crawl(["job-gpus-per-task"], job_id)
            if infrastructure == GATEWAY_PREFERRED_INFRASTRUCTURE:
                gateway.crawl(["job-gpus-per-task"], job_id)
            cluster.cancel(user, job_id)

            # sbatch --gres gpu:<n>
            job_id = cluster.submit(
                user,
                ["--partition", gpu_partition, "--gres ", f"gpu:{gpu_per_node}"],
            )
            slurmrestd.crawl(["job-gpus-gres"], job_id)
            if infrastructure == GATEWAY_PREFERRED_INFRASTRUCTURE:
                gateway.crawl(["job-gpus-gres"], job_id)
            cluster.cancel(user, job_id)

            # Allocate all GPUs on a node
            job_id = cluster.submit(
                user,
                [
                    "--partition",
                    gpu_partition,
                    "--gpus",
                    str(gpu_per_node),
                    "--nodes",
                    str(1),
                ],
            )
            node = cluster.job_nodes(job_id)[0]
            slurmrestd.crawl(["node-gpus-allocated"], node)
            if infrastructure == GATEWAY_PREFERRED_INFRASTRUCTURE:
                gateway.crawl(["node-gpus-allocated"], node)
            cluster.cancel(user, job_id)

            # Allocate some GPUs on a node
            job_id = cluster.submit(
                user, ["--partition", gpu_partition, "--gpus", str(1)]
            )
            node = cluster.job_nodes(job_id)[0]
            slurmrestd.crawl(["node-gpus-mixed"], node)
            if infrastructure == GATEWAY_PREFERRED_INFRASTRUCTURE:
                gateway.crawl(["node-gpus-mixed"], node)
            cluster.cancel(user, job_id)

            cluster.wait_idle(node)
            slurmrestd.crawl(["node-gpus-idle"], node)
            if infrastructure == GATEWAY_PREFERRED_INFRASTRUCTURE:
                gateway.crawl(["node-gpus-idle"], node)

        for _node in nodes["nodes"]:
            # Get node without GPU
            if not len(_node["gres"]):
                slurmrestd.crawl(["node-without-gpu"], _node["name"])
                if infrastructure == GATEWAY_PREFERRED_INFRASTRUCTURE:
                    gateway.crawl(["node-without-gpu"], _node["name"])
                break

        # Cancel all jobs
        cluster.cancel_all()


if __name__ == "__main__":
    main()
