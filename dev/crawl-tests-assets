#!/usr/bin/env python3
#
# Copyright (c) 2024 Rackslab
#
# This file is part of Slurm-web.
#
# SPDX-License-Identifier: GPL-3.0-or-later

import typing as t
from pathlib import Path
import sys
import json
import getpass
import socket
import shlex
import random
import os
import urllib
import logging

import requests
import paramiko

from rfl.log import setup_logger
from rfl.settings import RuntimeSettings
from rfl.settings.errors import (
    SettingsDefinitionError,
    SettingsOverrideError,
    SettingsSiteLoaderError,
)
from racksdb import RacksDB
from slurmweb.slurmrestd.unix import SlurmrestdUnixAdapter
from slurmweb.slurmrestd.auth import SlurmrestdAuthentifier
from slurmweb.metrics.db import SlurmwebMetricsDB
from slurmweb.version import get_version

logger = logging.getLogger("crawl-tests-assets")

DEBUG_FLAGS = ["slurmweb", "rfl", "werkzeug", "urllib3"]
DEV_HOST = "firehpc.dev.rackslab.io"
USER = getpass.getuser()
METRICS_PREFERRED_CLUSTER = "emulator"
# Map between infrastructure names and cluster names that are visible in Slurm-web.
MAP_CLUSTER_NAMES = {"emulator": "atlas"}
ADMIN_PASSWORD_ENV_VAR = "SLURMWEB_DEV_ADMIN_PASSWORD"


def slurmweb_cluster_name(infrastructure: str):
    if infrastructure in MAP_CLUSTER_NAMES:
        return MAP_CLUSTER_NAMES[infrastructure]
    return infrastructure


ASSETS = Path(__file__).parent.resolve() / ".." / "tests" / "assets"


def busy_node(node):
    """Return True if the given node is busy."""
    return "ALLOCATED" in node["state"] or "MIXED" in node["state"]


def crawl_prometheus(url: str, job: str) -> None:
    """Crawl and save test assets from Prometheus."""
    # Check assets directory
    assets_path = ASSETS / "prometheus"
    if not assets_path.exists():
        assets_path.mkdir(parents=True)

    # Save requests status
    status_file = assets_path / "status.json"
    if status_file.exists():
        with open(status_file) as fh:
            requests_statuses = json.load(fh)
    else:
        requests_statuses = {}

    headers = {}
    db = SlurmwebMetricsDB(url, job)

    for metric in ["nodes", "cores", "jobs"]:
        for _range in ["hour"]:
            dump_component_query(
                requests_statuses,
                url,
                f"{db.REQUEST_BASE_PATH}{db._query(metric, _range)}",
                headers,
                assets_path,
                f"{metric}-{_range}",
                prettify=False,
            )

    # query unexisting metric
    dump_component_query(
        requests_statuses,
        url,
        f"{db.REQUEST_BASE_PATH}{db._query('fail', 'hour')}",
        headers,
        assets_path,
        "unknown-metric",
    )

    # query unknown API path
    dump_component_query(
        requests_statuses,
        url,
        f"{db.REQUEST_BASE_PATH}/fail",
        headers,
        assets_path,
        "unknown-path",
    )

    # Save resulting status file
    with open(status_file, "w+") as fh:
        json.dump(requests_statuses, fh, indent=2)
        fh.write("\n")


def query_slurmrestd(
    session: requests.Session, headers: t.Dict[str, str], prefix: str, query: str
) -> t.Any:
    """Send GET HTTP request to slurmrestd and return JSON result. Raise RuntimeError in
    case of connection error or not JSON result."""
    try:
        response = session.get(f"{prefix}/{query}", headers=headers)
    except requests.exceptions.ConnectionError as err:
        raise RuntimeError(f"Unable to connect to slurmrestd: {err}")

    return response.text, response.headers.get("content-type"), response.status_code


def dump_slurmrestd_query(
    session: requests.Session,
    headers: t.Dict[str, str],
    requests_statuses,
    prefix: str,
    query: str,
    assets_path: Path,
    asset_name: str,
    skip_exist=True,
    limit_dump=0,
    limit_key=None,
) -> t.Any:
    """Send GET HTTP request to slurmrestd and save JSON result in assets directory."""

    if len(list(assets_path.glob(f"{asset_name}.*"))) and skip_exist:
        return

    text, content_type, status = query_slurmrestd(session, headers, prefix, query)

    if asset_name not in requests_statuses:
        requests_statuses[asset_name] = {}
    requests_statuses[asset_name]["content-type"] = content_type
    requests_statuses[asset_name]["status"] = status

    if content_type == "application/json":
        asset = assets_path / f"{asset_name}.json"
        data = json.loads(text)
    else:
        asset = assets_path / f"{asset_name}.txt"
        data = text

    if asset.exists():
        logger.warning("Asset %s already exists, skipping dump", asset)
    else:
        with open(asset, "w+") as fh:
            if content_type == "application/json":
                _data = data
                if limit_dump and limit_key:
                    _data = data.copy()
                    _data[limit_key] = _data[limit_key][:limit_dump]
                json.dump(_data, fh, indent=2)
            else:
                fh.write(data)
    return data


def crawl_slurmrestd(
    uri: urllib.parse.ParseResult, auth: SlurmrestdAuthentifier
) -> None:
    """Crawl and save test assets from slurmrestd on the given socket."""

    session = requests.Session()

    if uri.scheme == "unix":
        prefix = "http+unix://slurmrestd"
        session.mount(prefix, SlurmrestdUnixAdapter(uri.path))
    else:
        prefix = uri.geturl()

    api = "0.0.40"

    # Get Slurm version
    text, _, _ = query_slurmrestd(
        session, auth.headers(), prefix, f"/slurm/v{api}/ping"
    )
    ping = json.loads(text)
    release = ping["meta"]["slurm"]["release"]
    version = release.rsplit(".", 1)[0]
    logger.info("Slurm version: %s release: %s", version, release)

    # Check assets directory for this version
    assets_path = ASSETS / "slurmrestd" / version
    if not assets_path.exists():
        assets_path.mkdir(parents=True)

    # Save requests status
    status_file = assets_path / "status.json"
    if status_file.exists():
        with open(status_file) as fh:
            requests_statuses = json.load(fh)
    else:
        requests_statuses = {}

    # Download ping
    dump_slurmrestd_query(
        session,
        auth.headers(),
        requests_statuses,
        prefix,
        f"/slurm/v{api}/ping",
        assets_path,
        "slurm-ping",
    )

    # Download URL not found for both slurm and slurmdb
    dump_slurmrestd_query(
        session,
        auth.headers(),
        requests_statuses,
        prefix,
        f"/slurm/v{api}/not-found",
        assets_path,
        "slurm-not-found",
    )

    dump_slurmrestd_query(
        session,
        auth.headers(),
        requests_statuses,
        prefix,
        f"/slurmdb/v{api}/not-found",
        assets_path,
        "slurmdb-not-found",
    )

    if auth.method == "jwt":
        # if JWT auth, test missing headers
        dump_slurmrestd_query(
            session,
            {},
            requests_statuses,
            prefix,
            f"/slurm/v{api}/jobs",
            assets_path,
            "slurm-jwt-missing-headers",
        )
        # if JWT auth, test invalid headers
        dump_slurmrestd_query(
            session,
            {"X-SLURM-USER-FAIL": "tail"},
            requests_statuses,
            prefix,
            f"/slurm/v{api}/jobs",
            assets_path,
            "slurm-jwt-invalid-headers",
        )
        # if JWT auth, test invalid token
        dump_slurmrestd_query(
            session,
            {"X-SLURM-USER-NAME": auth.jwt_user, "X-SLURM-USER-TOKEN": "fail"},
            requests_statuses,
            prefix,
            f"/slurm/v{api}/jobs",
            assets_path,
            "slurm-jwt-invalid-token",
        )
        # if JWT auth and ability to generate token, test expired token
        if auth.jwt_mode == "auto":
            dump_slurmrestd_query(
                session,
                {
                    "X-SLURM-USER-NAME": auth.jwt_user,
                    "X-SLURM-USER-TOKEN": auth.jwt_manager.generate(
                        duration=-1, claimset={"sun": auth.jwt_user}
                    ),
                },
                requests_statuses,
                prefix,
                f"/slurm/v{api}/jobs",
                assets_path,
                "slurm-jwt-expired-token",
            )

    # Download jobs
    jobs = dump_slurmrestd_query(
        session,
        auth.headers(),
        requests_statuses,
        prefix,
        f"/slurm/v{api}/jobs",
        assets_path,
        "slurm-jobs",
        skip_exist=False,
        limit_dump=30,
        limit_key="jobs",
    )

    def dump_job_state(state: str):
        if state in _job["job_state"]:
            dump_slurmrestd_query(
                session,
                auth.headers(),
                requests_statuses,
                prefix,
                f"/slurm/v{api}/job/{_job['job_id']}",
                assets_path,
                f"slurm-job-{state.lower()}",
            )
            dump_slurmrestd_query(
                session,
                auth.headers(),
                requests_statuses,
                prefix,
                f"/slurmdb/v{api}/job/{_job['job_id']}",
                assets_path,
                f"slurmdb-job-{state.lower()}",
            )

    # Download specific jobs
    if not (len(jobs["jobs"])):
        logger.warning(
            "No jobs found in queue on socket %s, unable to crawl jobs data", socket
        )
    else:
        min_job_id = max_job_id = jobs["jobs"][0]["job_id"]

        for _job in jobs["jobs"]:
            if _job["job_id"] < min_job_id:
                min_job_id = _job["job_id"]
            if _job["job_id"] > max_job_id:
                max_job_id = _job["job_id"]

            for state in ["RUNNING", "PENDING", "COMPLETED", "FAILED", "TIMEOUT"]:
                dump_job_state(state)

        dump_slurmrestd_query(
            session,
            auth.headers(),
            requests_statuses,
            prefix,
            f"/slurm/v{api}/job/{min_job_id - 1}",
            assets_path,
            "slurm-job-archived",
        )
        dump_slurmrestd_query(
            session,
            auth.headers(),
            requests_statuses,
            prefix,
            f"/slurmdb/v{api}/job/{min_job_id - 1}",
            assets_path,
            "slurmdb-job-archived",
        )

        dump_slurmrestd_query(
            session,
            auth.headers(),
            requests_statuses,
            prefix,
            f"/slurm/v{api}/job/{max_job_id * 2}",
            assets_path,
            "slurm-job-unfound",
        )
        dump_slurmrestd_query(
            session,
            auth.headers(),
            requests_statuses,
            prefix,
            f"/slurmdb/v{api}/job/{max_job_id * 2}",
            assets_path,
            "slurmdb-job-unfound",
        )

    # Download nodes
    nodes = dump_slurmrestd_query(
        session,
        auth.headers(),
        requests_statuses,
        prefix,
        f"/slurm/v{api}/nodes",
        assets_path,
        "slurm-nodes",
        skip_exist=False,
        limit_dump=100,
        limit_key="nodes",
    )

    def dump_node_state():
        if state in _node["state"]:
            dump_slurmrestd_query(
                session,
                auth.headers(),
                requests_statuses,
                prefix,
                f"/slurm/v{api}/node/{_node['name']}",
                assets_path,
                f"slurm-node-{state.lower()}",
            )

    # Download specific node
    for _node in nodes["nodes"]:
        for state in ["IDLE", "MIXED", "ALLOCATED", "DOWN", "DRAINING", "DRAIN"]:
            dump_node_state()

    # Request node not found
    dump_slurmrestd_query(
        session,
        auth.headers(),
        requests_statuses,
        prefix,
        f"/slurm/v{api}/node/unexisting-node",
        assets_path,
        "slurm-node-unfound",
    )

    # Download partitions
    dump_slurmrestd_query(
        session,
        auth.headers(),
        requests_statuses,
        prefix,
        f"/slurm/v{api}/partitions",
        assets_path,
        "slurm-partitions",
    )

    # Download qos
    dump_slurmrestd_query(
        session,
        auth.headers(),
        requests_statuses,
        prefix,
        f"/slurmdb/v{api}/qos",
        assets_path,
        "slurm-qos",
    )

    # Download accounts
    dump_slurmrestd_query(
        session,
        auth.headers(),
        requests_statuses,
        prefix,
        f"/slurmdb/v{api}/accounts",
        assets_path,
        "slurm-accounts",
    )

    # Download reservations
    dump_slurmrestd_query(
        session,
        auth.headers(),
        requests_statuses,
        prefix,
        f"/slurm/v{api}/reservations",
        assets_path,
        "slurm-reservations",
    )

    # Save resulting status file
    with open(status_file, "w+") as fh:
        json.dump(requests_statuses, fh, indent=2, sort_keys=True)
        fh.write("\n")


def admin_user(cluster: str):
    """Return name of a user in admin group for the given cluster."""
    ssh_client = paramiko.SSHClient()
    ssh_client.load_host_keys(Path("~/.ssh/known_hosts").expanduser())
    logger.info("Connecting to development host %s", DEV_HOST)
    try:
        ssh_client.connect(DEV_HOST, username=USER)
    except socket.gaierror as err:
        logger.error("Unable to get address of %s: %s", DEV_HOST, str(err))
        sys.exit(1)
    except paramiko.ssh_exception.PasswordRequiredException as err:
        logger.error("Unable to connect on %s@%s: %s", USER, DEV_HOST, str(err))
        sys.exit(1)

    _, stdout, _ = ssh_client.exec_command(
        shlex.join(["firehpc", "status", "--cluster", cluster, "--json"])
    )
    cluster_status = json.loads(stdout.read())

    for group in cluster_status["groups"]:
        if group["name"] == "admin":
            return group["members"][0]

    logger.error("Unable to find user in admin group on cluster %s", cluster)
    sys.exit(1)


def gateway_url(dev_tmp_dir):
    """Return the gateway URL with service TCP port found in configuration."""
    # Load gateway configuration
    try:
        settings = RuntimeSettings.yaml_definition("conf/vendor/gateway.yml")
    except SettingsDefinitionError as err:
        logger.critical(err)
        sys.exit(1)
    try:
        settings.override_ini(dev_tmp_dir / "gateway.ini")
    except (SettingsSiteLoaderError, SettingsOverrideError) as err:
        logger.critical(err)
        sys.exit(1)
    # Compose and return the URL to the gateway
    return f"http://localhost:{settings.service.port}"


def user_token(url: str, user: str):
    """Ask user password interactively, authenticate on gateway and return
    authentication JWT."""

    try:
        password = os.environ[ADMIN_PASSWORD_ENV_VAR]
    except KeyError:
        logger.info(
            "Unable to read admin password from environment, opening interactive "
            "prompt."
        )
        password = getpass.getpass(prompt=f"Password for {user} on gateway: ")

    response = requests.post(
        f"{url}/api/login", json={"user": user, "password": password}
    )
    if response.status_code != 200:
        logger.error(
            "Authentication failed on gateway: %s", response.json()["description"]
        )
        sys.exit(1)

    return response.json()["token"]


def dump_component_query(
    requests_statuses,
    url: str,
    query: str,
    headers: str,
    assets_path: Path,
    asset_name: dict[int, str] | str,
    skip_exist: bool = True,
    prettify: bool = True,
    limit_dump=0,
    method: str = "GET",
    content: t.Optional[t.Dict] = None,
) -> t.Any:
    """Send GET HTTP request to Slurm-web component pointed by URL and save JSON result
    in assets directory."""
    if skip_exist:
        if isinstance(asset_name, dict):
            all_asset_exist = True
            for _asset_name in asset_name.values():
                if not len(list(assets_path.glob(f"{_asset_name}.*"))):
                    all_asset_exist = False
                    break
            if all_asset_exist:
                return
        else:
            assert isinstance(asset_name, str)
            if len(list(assets_path.glob(f"{asset_name}.*"))):
                return
    if method == "GET":
        response = requests.get(f"{url}{query}", headers=headers)
    elif method == "POST":
        kwargs = {}
        if content:
            kwargs["json"] = content
        response = requests.post(f"{url}{query}", headers=headers, **kwargs)
    else:
        raise RuntimeError(f"Unsupport request method {method}")
    if isinstance(asset_name, dict):
        _asset_name = asset_name[response.status_code]
    else:
        _asset_name = asset_name
    content_type = response.headers.get("content-type")
    if _asset_name not in requests_statuses:
        requests_statuses[_asset_name] = {}
    requests_statuses[_asset_name]["content-type"] = content_type
    requests_statuses[_asset_name]["status"] = response.status_code
    if content_type == "application/json":
        asset = assets_path / f"{_asset_name}.json"
        data = json.loads(response.text)
    else:
        asset = assets_path / f"{_asset_name}.txt"
        data = response.text

    if asset.exists():
        logger.warning("Asset %s already exists, skipping dump", asset)
    else:
        with open(asset, "w+") as fh:
            if asset.suffix == ".json":
                _data = data
                if limit_dump:
                    _data = _data[:limit_dump]
                fh.write(json.dumps(_data, indent=2 if prettify else None))
            else:
                fh.write(data)
    return data


def crawl_gateway(cluster: str, infrastructure: str, dev_tmp_dir: Path) -> str:
    """Crawl and save test assets from Slurm-web gateway component and return
    authentication JWT."""
    # Retrieve admin user account to connect
    user = admin_user(infrastructure)
    logger.info("Found user %s in group admin on cluster %s", user, cluster)

    # Get gateway HTTP base URL from configuration
    url = gateway_url(dev_tmp_dir)

    # Authenticate on gateway and get token
    token = user_token(url, user)
    headers = {"Authorization": f"Bearer {token}"}

    # Check assets directory
    assets_path = ASSETS / "gateway"
    if not assets_path.exists():
        assets_path.mkdir(parents=True)

    # Save requests status
    status_file = assets_path / "status.json"
    if status_file.exists():
        with open(status_file) as fh:
            requests_statuses = json.load(fh)
    else:
        requests_statuses = {}

    dump_component_query(
        requests_statuses, url, "/api/clusters", headers, assets_path, "clusters"
    )
    dump_component_query(
        requests_statuses, url, "/api/users", headers, assets_path, "users"
    )
    dump_component_query(
        requests_statuses,
        url,
        "/api/messages/login",
        headers,
        assets_path,
        {
            200: "message_login",
            404: "message_login_not_found",
            500: "message_login_error",
        },
    )

    dump_component_query(
        requests_statuses,
        url,
        f"/api/agents/{cluster}/stats",
        headers,
        assets_path,
        "stats",
    )

    jobs = dump_component_query(
        requests_statuses,
        url,
        f"/api/agents/{cluster}/jobs",
        headers,
        assets_path,
        "jobs",
        skip_exist=False,
        limit_dump=100,
    )

    if not (len(jobs)):
        logger.warning(
            "No jobs found in queue of cluster %s, unable to crawl jobs data", cluster
        )
    else:
        min_job_id = jobs[0]["job_id"]

        def dump_job_state() -> None:
            if state in _job["job_state"]:
                dump_component_query(
                    requests_statuses,
                    url,
                    f"/api/agents/{cluster}/job/{_job['job_id']}",
                    headers,
                    assets_path,
                    f"job-{state.lower()}",
                )

        for _job in jobs:
            if _job["job_id"] < min_job_id:
                min_job_id = _job["job_id"]
            for state in ["PENDING", "RUNNING", "COMPLETED", "FAILED", "TIMEOUT"]:
                dump_job_state()

        dump_component_query(
            requests_statuses,
            url,
            f"/api/agents/{cluster}/job/{min_job_id - 1}",
            headers,
            assets_path,
            "job-archived",
        )

    # FIXME: Download unknown job

    nodes = dump_component_query(
        requests_statuses,
        url,
        f"/api/agents/{cluster}/nodes",
        headers,
        assets_path,
        "nodes",
        skip_exist=False,
    )

    # Get jobs which have resources on any of the busy nodes
    try:
        random_busy_node = random.choice(list(filter(busy_node, nodes)))["name"]

        dump_component_query(
            requests_statuses,
            url,
            f"/api/agents/{cluster}/jobs?node={random_busy_node}",
            headers,
            assets_path,
            "jobs-node",
        )
    except IndexError:
        logger.warning("Unable to find busy node on gateway for cluster %s", cluster)

    def dump_node_state() -> None:
        if state in _node["state"]:
            dump_component_query(
                requests_statuses,
                url,
                f"/api/agents/{cluster}/node/{_node['name']}",
                headers,
                assets_path,
                f"node-{state.lower()}",
            )

    # Download specific node
    for _node in nodes:
        for state in ["IDLE", "MIXED", "ALLOCATED", "DOWN", "DRAINING", "DRAINED"]:
            dump_node_state()

    # FIXME: download unknown node

    dump_component_query(
        requests_statuses,
        url,
        f"/api/agents/{cluster}/partitions",
        headers,
        assets_path,
        "partitions",
    )
    dump_component_query(
        requests_statuses,
        url,
        f"/api/agents/{cluster}/qos",
        headers,
        assets_path,
        "qos",
    )
    dump_component_query(
        requests_statuses,
        url,
        f"/api/agents/{cluster}/reservations",
        headers,
        assets_path,
        "reservations",
    )
    dump_component_query(
        requests_statuses,
        url,
        f"/api/agents/{cluster}/accounts",
        headers,
        assets_path,
        "accounts",
    )

    # RacksDB infrastructure diagram
    dump_component_query(
        requests_statuses,
        url,
        f"/api/agents/{cluster}/racksdb/draw/infrastructure/{infrastructure}.png?coordinates",
        headers,
        assets_path,
        "racksdb-draw-coordinates",
        method="POST",
        content={
            # FIXME: Retrieve these RacksDB request parameters from Slurm-web code base
            # to avoid duplication.
            "general": {"pixel_perfect": True},
            "dimensions": {"width": 1000, "height": 300},
            "infrastructure": {"equipment_labels": False, "ghost_unselected": True},
        },
    )
    # metrics
    for metric in ["nodes", "cores", "jobs"]:
        for _range in ["hour"]:
            dump_component_query(
                requests_statuses,
                url,
                f"/api/agents/{cluster}/metrics/{metric}?range={_range}",
                headers,
                assets_path,
                f"metrics-{metric}-{_range}",
                prettify=False,
            )

    # Save resulting status file
    with open(status_file, "w+") as fh:
        json.dump(requests_statuses, fh, indent=2, sort_keys=True)
        fh.write("\n")

    return token


def crawl_agent(port: int, token: str, metrics: bool) -> None:
    """Crawl and save test assets from Slurm-web agent component."""
    # Compose and return the URL to the gateway
    url = f"http://localhost:{port}"

    # Check assets directory
    assets_path = ASSETS / "agent"
    if not assets_path.exists():
        assets_path.mkdir(parents=True)

    # Save requests status
    status_file = assets_path / "status.json"
    if status_file.exists():
        with open(status_file) as fh:
            requests_statuses = json.load(fh)
    else:
        requests_statuses = {}

    headers = {"Authorization": f"Bearer {token}"}

    dump_component_query(
        requests_statuses, url, f"/v{get_version()}/info", headers, assets_path, "info"
    )
    dump_component_query(
        requests_statuses,
        url,
        f"/v{get_version()}/permissions",
        headers,
        assets_path,
        "permissions",
    )
    dump_component_query(
        requests_statuses,
        url,
        f"/v{get_version()}/stats",
        headers,
        assets_path,
        "stats",
    )
    dump_component_query(
        requests_statuses,
        url,
        f"/v{get_version()}/jobs",
        headers,
        assets_path,
        "jobs",
        limit_dump=100,
    )
    nodes = dump_component_query(
        requests_statuses,
        url,
        f"/v{get_version()}/nodes",
        headers,
        assets_path,
        "nodes",
        skip_exist=False,
    )

    # Get jobs which have resources on any of the busy nodes
    try:
        random_busy_node = random.choice(list(filter(busy_node, nodes)))["name"]
        dump_component_query(
            requests_statuses,
            url,
            f"/v{get_version()}/jobs?node={random_busy_node}",
            headers,
            assets_path,
            "jobs-node",
        )
    except IndexError:
        logger.warning("Unable to find busy node on agent")

    dump_component_query(
        requests_statuses,
        url,
        f"/v{get_version()}/partitions",
        headers,
        assets_path,
        "partitions",
    )
    dump_component_query(
        requests_statuses, url, f"/v{get_version()}/qos", headers, assets_path, "qos"
    )
    dump_component_query(
        requests_statuses,
        url,
        f"/v{get_version()}/reservations",
        headers,
        assets_path,
        "reservations",
    )
    dump_component_query(
        requests_statuses,
        url,
        f"/v{get_version()}/accounts",
        headers,
        assets_path,
        "accounts",
    )

    # metrics
    if metrics:
        for metric in ["nodes", "cores", "jobs"]:
            for _range in ["hour"]:
                dump_component_query(
                    requests_statuses,
                    url,
                    f"/v{get_version()}/metrics/{metric}?range={_range}",
                    headers,
                    assets_path,
                    f"metrics-{metric}-{_range}",
                    prettify=False,
                )

    # FIXME: Download unknown job/node
    # Save resulting status file
    with open(status_file, "w+") as fh:
        json.dump(requests_statuses, fh, indent=2, sort_keys=True)
        fh.write("\n")


def main() -> None:
    """Crawl and save test assets from Slurm-web gateway, agent and slurmrestd."""

    # Setup logger
    setup_logger(
        debug=True,
        log_flags=["ALL"],
        debug_flags=DEBUG_FLAGS,
    )

    # Search for slurm-web development environment temporary directory
    dev_tmp_dirs = list(Path("/tmp").glob("slurm-web-*"))
    try:
        assert len(dev_tmp_dirs) == 1
    except AssertionError:
        logger.error(
            "Unexpectedly found %d Slurm-web development temporary directories",
            len(dev_tmp_dirs),
        )
        sys.exit(1)
    dev_tmp_dir = dev_tmp_dirs[0]
    logger.info(
        "Slurm-web development environment temporary directory: %s", dev_tmp_dir
    )

    # Load cluster list from RacksDB database
    db = RacksDB.load(db="dev/firehpc/db", schema="../RacksDB/schemas/racksdb.yml")
    logger.info("List of clusters: %s", db.infrastructures.keys())

    gateway_infrastructure = list(
        db.infrastructures.filter(name=METRICS_PREFERRED_CLUSTER)
    )[0].name

    # Crawl gateway and get bearer token
    token = crawl_gateway(
        slurmweb_cluster_name(gateway_infrastructure),
        gateway_infrastructure,
        dev_tmp_dir,
    )

    for cluster in db.infrastructures.keys():
        # Load agent configuration
        try:
            settings = RuntimeSettings.yaml_definition("conf/vendor/agent.yml")
        except SettingsDefinitionError as err:
            logger.critical(err)
            sys.exit(1)
        try:
            settings.override_ini(dev_tmp_dir / f"agent-{cluster}.ini")
        except (SettingsSiteLoaderError, SettingsOverrideError) as err:
            logger.critical(err)
            sys.exit(1)

        crawl_metrics = cluster == METRICS_PREFERRED_CLUSTER

        # Crawl agent
        crawl_agent(settings.service.port, token, metrics=crawl_metrics)

        # Crawl prometheus
        if crawl_metrics:
            crawl_prometheus(settings.metrics.host.geturl(), settings.metrics.job)

        auth = SlurmrestdAuthentifier(
            settings.slurmrestd.auth,
            settings.slurmrestd.jwt_mode,
            settings.slurmrestd.jwt_user,
            settings.slurmrestd.jwt_key,
            settings.slurmrestd.jwt_lifespan,
            settings.slurmrestd.jwt_token,
        )

        # Crawl slurmrestd
        try:
            crawl_slurmrestd(settings.slurmrestd.uri, auth)
        except RuntimeError as err:
            logger.error(
                "Unable to crawl slurmrestd data from cluster %s: %s", cluster, err
            )


if __name__ == "__main__":
    main()
