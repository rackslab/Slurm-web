#!/usr/bin/env python3
#
# Copyright (c) 2024 Rackslab
#
# This file is part of Slurm-web.
#
# SPDX-License-Identifier: GPL-3.0-or-later

from typing import Any
from pathlib import Path
import sys
import json
import getpass
import socket
import shlex
import random
import logging

import requests
import paramiko

from rfl.log import setup_logger
from rfl.settings import RuntimeSettings
from rfl.settings.errors import (
    SettingsDefinitionError,
    SettingsOverrideError,
    SettingsSiteLoaderError,
)
from racksdb import RacksDB
from slurmweb.slurmrestd.unix import SlurmrestdUnixAdapter
from slurmweb.metrics.db import SlurmwebMetricsDB
from slurmweb.version import get_version

logger = logging.getLogger("crawl-tests-assets")

DEBUG_FLAGS = ["slurmweb", "rfl", "werkzeug", "urllib3"]
DEV_HOST = "firehpc.dev.rackslab.io"
USER = getpass.getuser()
METRICS_PREFERRED_CLUSTER = "emulator"
# Map between infrastructure names and cluster names that are visible in Slurm-web.
MAP_CLUSTER_NAMES = {"emulator": "atlas"}


def slurmweb_cluster_name(infrastructure: str):
    if infrastructure in MAP_CLUSTER_NAMES:
        return MAP_CLUSTER_NAMES[infrastructure]
    return infrastructure


ASSETS = Path(__file__).parent.resolve() / ".." / "slurmweb" / "tests" / "assets"


def busy_node(node):
    """Return True if the given node is busy."""
    return "ALLOCATED" in node["state"] or "MIXED" in node["state"]


def crawl_prometheus(url: str, job: str) -> None:
    """Crawl and save test assets from Prometheus."""
    # Check assets directory
    assets_path = ASSETS / "prometheus"
    if not assets_path.exists():
        assets_path.mkdir(parents=True)

    # Save requests status
    status_file = assets_path / "status.json"
    if status_file.exists():
        with open(status_file) as fh:
            requests_statuses = json.load(fh)
    else:
        requests_statuses = {}

    headers = {}
    db = SlurmwebMetricsDB(url, job)

    for metric in ["nodes", "cores", "jobs"]:
        for _range in ["hour"]:
            dump_component_query(
                requests_statuses,
                url,
                f"{db.REQUEST_BASE_PATH}{db._query(metric, _range)}",
                headers,
                assets_path,
                f"{metric}-{_range}",
                prettify=False,
            )

    # query unexisting metric
    dump_component_query(
        requests_statuses,
        url,
        f"{db.REQUEST_BASE_PATH}{db._query('fail', 'hour')}",
        headers,
        assets_path,
        "unknown-metric",
    )

    # query unknown API path
    dump_component_query(
        requests_statuses,
        url,
        f"{db.REQUEST_BASE_PATH}/fail",
        headers,
        assets_path,
        "unknown-path",
    )

    # Save resulting status file
    with open(status_file, "w+") as fh:
        json.dump(requests_statuses, fh, indent=2)
        fh.write("\n")


def query_slurmrestd(session: requests.Session, prefix: str, query: str) -> Any:
    """Send GET HTTP request to slurmrestd and return JSON result. Raise RuntimeError in
    case of connection error or not JSON result."""
    try:
        response = session.get(f"{prefix}/{query}")
    except requests.exceptions.ConnectionError as err:
        raise RuntimeError(f"Unable to connect to slurmrestd: {err}")

    return response.text, response.headers.get("content-type"), response.status_code


def dump_slurmrestd_query(
    session: requests.Session,
    requests_statuses,
    prefix: str,
    query: str,
    assets_path: Path,
    asset_name: str,
    skip_exist=True,
    limit_dump=0,
    limit_key=None,
) -> Any:
    """Send GET HTTP request to slurmrestd and save JSON result in assets directory."""

    if len(list(assets_path.glob(f"{asset_name}.*"))) and skip_exist:
        return

    text, content_type, status = query_slurmrestd(session, prefix, query)

    if asset_name not in requests_statuses:
        requests_statuses[asset_name] = {}
    requests_statuses[asset_name]["content-type"] = content_type
    requests_statuses[asset_name]["status"] = status

    if content_type == "application/json":
        asset = assets_path / f"{asset_name}.json"
        data = json.loads(text)
    else:
        asset = assets_path / f"{asset_name}.txt"
        data = text

    if asset.exists():
        logger.warning("Asset %s already exists, skipping dump", asset)
    else:
        with open(asset, "w+") as fh:
            if content_type == "application/json":
                _data = data
                if limit_dump and limit_key:
                    _data = data.copy()
                    _data[limit_key] = _data[limit_key][:limit_dump]
                json.dump(_data, fh, indent=2)
            else:
                fh.write(data)
    return data


def crawl_slurmrestd(socket: Path) -> None:
    """Crawl and save test assets from slurmrestd on the given socket."""

    session = requests.Session()
    prefix = "http+unix://slurmrestd/"
    api = "0.0.40"
    session.mount(prefix, SlurmrestdUnixAdapter(socket))

    # Get Slurm version
    text, _, _ = query_slurmrestd(session, prefix, f"/slurm/v{api}/ping")
    ping = json.loads(text)
    release = ping["meta"]["slurm"]["release"]
    version = release.rsplit(".", 1)[0]
    logger.info("Slurm version: %s release: %s", version, release)

    # Check assets directory for this version
    assets_path = ASSETS / "slurmrestd" / version
    if not assets_path.exists():
        assets_path.mkdir(parents=True)

    # Save requests status
    status_file = assets_path / "status.json"
    if status_file.exists():
        with open(status_file) as fh:
            requests_statuses = json.load(fh)
    else:
        requests_statuses = {}

    # Download ping
    dump_slurmrestd_query(
        session,
        requests_statuses,
        prefix,
        f"/slurm/v{api}/ping",
        assets_path,
        "slurm-ping",
    )

    # Download URL not found for both slurm and slurmdb
    dump_slurmrestd_query(
        session,
        requests_statuses,
        prefix,
        f"/slurm/v{api}/not-found",
        assets_path,
        "slurm-not-found",
    )

    dump_slurmrestd_query(
        session,
        requests_statuses,
        prefix,
        f"/slurmdb/v{api}/not-found",
        assets_path,
        "slurmdb-not-found",
    )

    # Download jobs
    jobs = dump_slurmrestd_query(
        session,
        requests_statuses,
        prefix,
        f"/slurm/v{api}/jobs",
        assets_path,
        "slurm-jobs",
        skip_exist=False,
        limit_dump=30,
        limit_key="jobs",
    )

    def dump_job_state(state: str):
        if state in _job["job_state"]:
            dump_slurmrestd_query(
                session,
                requests_statuses,
                prefix,
                f"/slurm/v{api}/job/{_job['job_id']}",
                assets_path,
                f"slurm-job-{state.lower()}",
            )
            dump_slurmrestd_query(
                session,
                requests_statuses,
                prefix,
                f"/slurmdb/v{api}/job/{_job['job_id']}",
                assets_path,
                f"slurmdb-job-{state.lower()}",
            )

    # Download specific jobs
    if not (len(jobs["jobs"])):
        logger.warning(
            "No jobs found in queue on socket %s, unable to crawl jobs data", socket
        )
    else:

        min_job_id = max_job_id = jobs["jobs"][0]["job_id"]

        for _job in jobs["jobs"]:
            if _job["job_id"] < min_job_id:
                min_job_id = _job["job_id"]
            if _job["job_id"] > max_job_id:
                max_job_id = _job["job_id"]

            for state in ["RUNNING", "PENDING", "COMPLETED"]:
                dump_job_state(state)

        dump_slurmrestd_query(
            session,
            requests_statuses,
            prefix,
            f"/slurm/v{api}/job/{min_job_id-1}",
            assets_path,
            "slurm-job-archived",
        )
        dump_slurmrestd_query(
            session,
            requests_statuses,
            prefix,
            f"/slurmdb/v{api}/job/{min_job_id-1}",
            assets_path,
            "slurmdb-job-archived",
        )

        dump_slurmrestd_query(
            session,
            requests_statuses,
            prefix,
            f"/slurm/v{api}/job/{max_job_id*2}",
            assets_path,
            "slurm-job-unfound",
        )
        dump_slurmrestd_query(
            session,
            requests_statuses,
            prefix,
            f"/slurmdb/v{api}/job/{max_job_id*2}",
            assets_path,
            "slurmdb-job-unfound",
        )

    # Download nodes
    nodes = dump_slurmrestd_query(
        session,
        requests_statuses,
        prefix,
        f"/slurm/v{api}/nodes",
        assets_path,
        "slurm-nodes",
        skip_exist=False,
        limit_dump=100,
        limit_key="nodes",
    )

    def dump_node_state():
        if state in _node["state"]:
            dump_slurmrestd_query(
                session,
                requests_statuses,
                prefix,
                f"/slurm/v{api}/node/{_node['name']}",
                assets_path,
                f"slurm-node-{state.lower()}",
            )

    # Download specific node
    for _node in nodes["nodes"]:
        for state in ["IDLE", "MIXED", "ALLOCATED", "DOWN", "DRAINING", "DRAIN"]:
            dump_node_state()

    # Request node not found
    dump_slurmrestd_query(
        session,
        requests_statuses,
        prefix,
        f"/slurm/v{api}/node/unexisting-node",
        assets_path,
        "slurm-node-unfound",
    )

    # Download partitions
    dump_slurmrestd_query(
        session,
        requests_statuses,
        prefix,
        f"/slurm/v{api}/partitions",
        assets_path,
        "slurm-partitions",
    )

    # Download qos
    dump_slurmrestd_query(
        session,
        requests_statuses,
        prefix,
        f"/slurmdb/v{api}/qos",
        assets_path,
        "slurm-qos",
    )

    # Download accounts
    dump_slurmrestd_query(
        session,
        requests_statuses,
        prefix,
        f"/slurmdb/v{api}/accounts",
        assets_path,
        "slurm-accounts",
    )

    # Download reservations
    dump_slurmrestd_query(
        session,
        requests_statuses,
        prefix,
        f"/slurm/v{api}/reservations",
        assets_path,
        "slurm-reservations",
    )

    # Save resulting status file
    with open(status_file, "w+") as fh:
        json.dump(requests_statuses, fh, indent=2, sort_keys=True)
        fh.write("\n")


def admin_user(cluster: str):
    """Return name of a user in admin group for the given cluster."""
    ssh_client = paramiko.SSHClient()
    ssh_client.load_host_keys(Path("~/.ssh/known_hosts").expanduser())
    logger.info("Connecting to development host %s", DEV_HOST)
    try:
        ssh_client.connect(DEV_HOST, username=USER)
    except socket.gaierror as err:
        logger.error("Unable to get address of %s: %s", DEV_HOST, str(err))
        sys.exit(1)
    except paramiko.ssh_exception.PasswordRequiredException as err:
        logger.error("Unable to connect on %s@%s: %s", USER, DEV_HOST, str(err))
        sys.exit(1)

    _, stdout, _ = ssh_client.exec_command(
        shlex.join(["firehpc", "status", "--cluster", cluster, "--json"])
    )
    cluster_status = json.loads(stdout.read())

    for group in cluster_status["groups"]:
        if group["name"] == "admin":
            return group["members"][0]

    logger.error("Unable to find user in admin group on cluster %s", cluster)
    sys.exit(1)


def gateway_url(dev_tmp_dir):
    """Return the gateway URL with service TCP port found in configuration."""
    # Load gateway configuration
    try:
        settings = RuntimeSettings.yaml_definition("conf/vendor/gateway.yml")
    except SettingsDefinitionError as err:
        logger.critical(err)
        sys.exit(1)
    try:
        settings.override_ini(dev_tmp_dir / "gateway.ini")
    except (SettingsSiteLoaderError, SettingsOverrideError) as err:
        logger.critical(err)
        sys.exit(1)
    # Compose and return the URL to the gateway
    return f"http://localhost:{settings.service.port}"


def user_token(url: str, user: str):
    """Ask user password interactively, authenticate on gateway and return
    authentication JWT."""
    password = getpass.getpass(prompt=f"Password for {user} on gateway: ")

    response = requests.post(
        f"{url}/api/login", json={"user": user, "password": password}
    )
    if response.status_code != 200:
        logger.error(
            "Authentication failed on gateway: %s", response.json()["description"]
        )
        sys.exit(1)

    return response.json()["token"]


def dump_component_query(
    requests_statuses,
    url: str,
    query: str,
    headers: str,
    assets_path: Path,
    asset_name: dict[int, str] | str,
    skip_exist: bool = True,
    prettify: bool = True,
    limit_dump=0,
) -> Any:
    """Send GET HTTP request to Slurm-web component pointed by URL and save JSON result
    in assets directory."""
    if skip_exist:
        if isinstance(asset_name, dict):
            all_asset_exist = True
            for _asset_name in asset_name.values():
                if not len(list(assets_path.glob(f"{_asset_name}.*"))):
                    all_asset_exist = False
                    break
            if all_asset_exist:
                return
        else:
            assert isinstance(asset_name, str)
            if len(list(assets_path.glob(f"{asset_name}.*"))):
                return
    response = requests.get(f"{url}{query}", headers=headers)
    if isinstance(asset_name, dict):
        _asset_name = asset_name[response.status_code]
    else:
        _asset_name = asset_name
    content_type = response.headers.get("content-type")
    if _asset_name not in requests_statuses:
        requests_statuses[_asset_name] = {}
    requests_statuses[_asset_name]["content-type"] = content_type
    requests_statuses[_asset_name]["status"] = response.status_code
    if content_type == "application/json":
        asset = assets_path / f"{_asset_name}.json"
        data = json.loads(response.text)
    else:
        asset = assets_path / f"{_asset_name}.txt"
        data = response.text

    if asset.exists():
        logger.warning("Asset %s already exists, skipping dump", asset)
    else:
        with open(asset, "w+") as fh:
            if asset.suffix == ".json":
                _data = data
                if limit_dump:
                    _data = _data[:limit_dump]
                fh.write(json.dumps(_data, indent=2 if prettify else None))
            else:
                fh.write(data)
    return data


def crawl_gateway(cluster: str, infrastructure: str, dev_tmp_dir: Path) -> str:
    """Crawl and save test assets from Slurm-web gateway component and return
    authentication JWT."""
    # Retrieve admin user account to connect
    user = admin_user(infrastructure)
    logger.info("Found user %s in group admin on cluster %s", user, cluster)

    # Get gateway HTTP base URL from configuration
    url = gateway_url(dev_tmp_dir)

    # Authenticate on gateway and get token
    token = user_token(url, user)
    headers = {"Authorization": f"Bearer {token}"}

    # Check assets directory
    assets_path = ASSETS / "gateway"
    if not assets_path.exists():
        assets_path.mkdir(parents=True)

    # Save requests status
    status_file = assets_path / "status.json"
    if status_file.exists():
        with open(status_file) as fh:
            requests_statuses = json.load(fh)
    else:
        requests_statuses = {}

    dump_component_query(
        requests_statuses, url, "/api/clusters", headers, assets_path, "clusters"
    )
    dump_component_query(
        requests_statuses, url, "/api/users", headers, assets_path, "users"
    )
    dump_component_query(
        requests_statuses,
        url,
        "/api/messages/login",
        headers,
        assets_path,
        {
            200: "message_login",
            404: "message_login_not_found",
            500: "message_login_error",
        },
    )

    dump_component_query(
        requests_statuses,
        url,
        f"/api/agents/{cluster}/stats",
        headers,
        assets_path,
        "stats",
    )

    jobs = dump_component_query(
        requests_statuses,
        url,
        f"/api/agents/{cluster}/jobs",
        headers,
        assets_path,
        "jobs",
        skip_exist=False,
        limit_dump=100,
    )

    if not (len(jobs)):
        logger.warning(
            "No jobs found in queue of cluster %s, unable to crawl jobs data", cluster
        )
    else:
        min_job_id = jobs[0]["job_id"]

        def dump_job_state() -> None:
            if state in _job["job_state"]:
                dump_component_query(
                    requests_statuses,
                    url,
                    f"/api/agents/{cluster}/job/{_job['job_id']}",
                    headers,
                    assets_path,
                    f"job-{state.lower()}",
                )

        for _job in jobs:
            if _job["job_id"] < min_job_id:
                min_job_id = _job["job_id"]
            for state in ["PENDING", "RUNNING", "COMPLETED"]:
                dump_job_state()

        dump_component_query(
            requests_statuses,
            url,
            f"/api/agents/{cluster}/job/{min_job_id-1}",
            headers,
            assets_path,
            "job-archived",
        )

    # FIXME: Download unknown job

    nodes = dump_component_query(
        requests_statuses,
        url,
        f"/api/agents/{cluster}/nodes",
        headers,
        assets_path,
        "nodes",
        skip_exist=False,
    )

    # Get jobs which have resources on any of the busy nodes
    try:
        random_busy_node = random.choice(list(filter(busy_node, nodes)))["name"]

        dump_component_query(
            requests_statuses,
            url,
            f"/api/agents/{cluster}/jobs?node={random_busy_node}",
            headers,
            assets_path,
            "jobs-node",
        )
    except IndexError:
        logger.warning("Unable to find busy node on gateway for cluster %s", cluster)

    def dump_node_state() -> None:
        if state in _node["state"]:
            dump_component_query(
                requests_statuses,
                url,
                f"/api/agents/{cluster}/node/{_node['name']}",
                headers,
                assets_path,
                f"node-{state.lower()}",
            )

    # Download specific node
    for _node in nodes:
        for state in ["IDLE", "MIXED", "ALLOCATED", "DOWN", "DRAINING", "DRAINED"]:
            dump_node_state()

    # FIXME: download unknown node

    dump_component_query(
        requests_statuses,
        url,
        f"/api/agents/{cluster}/partitions",
        headers,
        assets_path,
        "partitions",
    )
    dump_component_query(
        requests_statuses,
        url,
        f"/api/agents/{cluster}/qos",
        headers,
        assets_path,
        "qos",
    )
    dump_component_query(
        requests_statuses,
        url,
        f"/api/agents/{cluster}/reservations",
        headers,
        assets_path,
        "reservations",
    )
    dump_component_query(
        requests_statuses,
        url,
        f"/api/agents/{cluster}/accounts",
        headers,
        assets_path,
        "accounts",
    )

    # metrics
    for metric in ["nodes", "cores", "jobs"]:
        for _range in ["hour"]:
            dump_component_query(
                requests_statuses,
                url,
                f"/api/agents/{cluster}/metrics/{metric}?range={_range}",
                headers,
                assets_path,
                f"metrics-{metric}-{_range}",
                prettify=False,
            )

    # Save resulting status file
    with open(status_file, "w+") as fh:
        json.dump(requests_statuses, fh, indent=2, sort_keys=True)
        fh.write("\n")

    return token


def crawl_agent(port: int, token: str, metrics: bool) -> None:
    """Crawl and save test assets from Slurm-web agent component."""
    # Compose and return the URL to the gateway
    url = f"http://localhost:{port}"

    # Check assets directory
    assets_path = ASSETS / "agent"
    if not assets_path.exists():
        assets_path.mkdir(parents=True)

    # Save requests status
    status_file = assets_path / "status.json"
    if status_file.exists():
        with open(status_file) as fh:
            requests_statuses = json.load(fh)
    else:
        requests_statuses = {}

    headers = {"Authorization": f"Bearer {token}"}

    dump_component_query(
        requests_statuses, url, f"/v{get_version()}/info", headers, assets_path, "info"
    )
    dump_component_query(
        requests_statuses,
        url,
        f"/v{get_version()}/permissions",
        headers,
        assets_path,
        "permissions",
    )
    dump_component_query(
        requests_statuses,
        url,
        f"/v{get_version()}/stats",
        headers,
        assets_path,
        "stats",
    )
    dump_component_query(
        requests_statuses,
        url,
        f"/v{get_version()}/jobs",
        headers,
        assets_path,
        "jobs",
        limit_dump=100,
    )
    nodes = dump_component_query(
        requests_statuses,
        url,
        f"/v{get_version()}/nodes",
        headers,
        assets_path,
        "nodes",
        skip_exist=False,
    )

    # Get jobs which have resources on any of the busy nodes
    try:
        random_busy_node = random.choice(list(filter(busy_node, nodes)))["name"]
        dump_component_query(
            requests_statuses,
            url,
            f"/v{get_version()}/jobs?node={random_busy_node}",
            headers,
            assets_path,
            "jobs-node",
        )
    except IndexError:
        logger.warning("Unable to find busy node on agent")

    dump_component_query(
        requests_statuses,
        url,
        f"/v{get_version()}/partitions",
        headers,
        assets_path,
        "partitions",
    )
    dump_component_query(
        requests_statuses, url, f"/v{get_version()}/qos", headers, assets_path, "qos"
    )
    dump_component_query(
        requests_statuses,
        url,
        f"/v{get_version()}/reservations",
        headers,
        assets_path,
        "reservations",
    )
    dump_component_query(
        requests_statuses,
        url,
        f"/v{get_version()}/accounts",
        headers,
        assets_path,
        "accounts",
    )

    # metrics
    if metrics:
        for metric in ["nodes", "cores", "jobs"]:
            for _range in ["hour"]:
                dump_component_query(
                    requests_statuses,
                    url,
                    f"/v{get_version()}/metrics/{metric}?range={_range}",
                    headers,
                    assets_path,
                    f"metrics-{metric}-{_range}",
                    prettify=False,
                )

    # FIXME: Download unknown job/node
    # Save resulting status file
    with open(status_file, "w+") as fh:
        json.dump(requests_statuses, fh, indent=2, sort_keys=True)
        fh.write("\n")


def main() -> None:
    """Crawl and save test assets from Slurm-web gateway, agent and slurmrestd."""

    # Setup logger
    setup_logger(
        debug=True,
        log_flags=["ALL"],
        debug_flags=DEBUG_FLAGS,
    )

    # Search for slurm-web development environment temporary directory
    dev_tmp_dirs = list(Path("/tmp").glob("slurm-web-*"))
    try:
        assert len(dev_tmp_dirs) == 1
    except AssertionError:
        logger.error(
            "Unexpectedly found %d Slurm-web development temporary directories",
            len(dev_tmp_dirs),
        )
        sys.exit(1)
    dev_tmp_dir = dev_tmp_dirs[0]
    logger.info(
        "Slurm-web development environment temporary directory: %s", dev_tmp_dir
    )

    # Load cluster list from RacksDB database
    db = RacksDB.load(db="dev/firehpc/db", schema="../RacksDB/schemas/racksdb.yml")
    logger.info("List of clusters: %s", db.infrastructures.keys())

    gateway_infrastructure = list(
        db.infrastructures.filter(name=METRICS_PREFERRED_CLUSTER)
    )[0].name

    # Crawl gateway and get bearer token
    token = crawl_gateway(
        slurmweb_cluster_name(gateway_infrastructure),
        gateway_infrastructure,
        dev_tmp_dir,
    )

    for cluster in db.infrastructures.keys():
        # Load agent configuration
        try:
            settings = RuntimeSettings.yaml_definition("conf/vendor/agent.yml")
        except SettingsDefinitionError as err:
            logger.critical(err)
            sys.exit(1)
        try:
            settings.override_ini(dev_tmp_dir / f"agent-{cluster}.ini")
        except (SettingsSiteLoaderError, SettingsOverrideError) as err:
            logger.critical(err)
            sys.exit(1)

        crawl_metrics = cluster == METRICS_PREFERRED_CLUSTER

        # Crawl agent
        crawl_agent(settings.service.port, token, metrics=crawl_metrics)

        # Crawl prometheus
        if crawl_metrics:
            crawl_prometheus(settings.metrics.host.geturl(), settings.metrics.job)

        # Crawl slurmrestd
        try:
            crawl_slurmrestd(settings.slurmrestd.socket)
        except RuntimeError as err:
            logger.error(
                "Unable to crawl slurmrestd data from cluster %s: %s", cluster, err
            )


if __name__ == "__main__":
    main()
