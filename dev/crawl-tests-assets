#!/usr/bin/env python3
#
# Copyright (c) 2024 Rackslab
#
# This file is part of Slurm-web.
#
# SPDX-License-Identifier: MIT

from pathlib import Path
import sys
import getpass
import logging

from rfl.log import setup_logger

from crawler.lib import (
    load_settings,
    DevelopmentHostClient,
    DevelopmentHostConnectionError,
    DevelopmentHostCluster,
)
from crawler.slurmrestd import SlurmrestdCrawler
from crawler.agent import crawl_agent
from crawler.gateway import slurmweb_token, GatewayCrawler
from crawler.prometheus import crawl_prometheus

from racksdb import RacksDB
from slurmweb.slurmrestd.auth import SlurmrestdAuthentifier

logger = logging.getLogger("crawl-tests-assets")

DEBUG_FLAGS = ["slurmweb", "rfl", "werkzeug", "urllib3", "crawler"]
DEV_HOST = "firehpc.dev.rackslab.io"
USER = getpass.getuser()
GATEWAY_PREFERRED_INFRASTRUCTURE = "tiny"
METRICS_PREFERRED_INFRASTRUCTURE = "emulator"
# Map between infrastructure names and cluster names that are visible in Slurm-web.
MAP_CLUSTER_NAMES = {"emulator": "atlas"}

ASSETS_ROOT = Path("tests/assets")


def slurmweb_cluster_name(infrastructure: str):
    return MAP_CLUSTER_NAMES.get(infrastructure, infrastructure)


def main() -> None:
    """Crawl and save test assets from Slurm-web gateway, agent and slurmrestd."""

    # FIXME: support possibility to restrict cluster

    # Setup logger
    setup_logger(
        debug=True,
        log_flags=["ALL"],
        debug_flags=DEBUG_FLAGS,
    )

    # Search for slurm-web development environment temporary directory
    dev_tmp_dirs = list(Path("/tmp").glob("slurm-web-*"))
    try:
        assert len(dev_tmp_dirs) == 1
    except AssertionError:
        logger.error(
            "Unexpectedly found %d Slurm-web development temporary directories",
            len(dev_tmp_dirs),
        )
        sys.exit(1)
    dev_tmp_dir = dev_tmp_dirs[0]
    logger.info(
        "Slurm-web development environment temporary directory: %s", dev_tmp_dir
    )

    # Load cluster list from RacksDB database
    db = RacksDB.load(db="dev/firehpc/db", schema="../RacksDB/schemas/racksdb.yml")
    logger.info("List of clusters: %s", db.infrastructures.keys())

    dev_host = DevelopmentHostClient(DEV_HOST, USER)
    try:
        dev_host.connect()
    except DevelopmentHostConnectionError as err:
        logger.error(err)
        sys.exit(1)

    # Get Slurm-web JWT for authentication on gateway and agent
    token = slurmweb_token(
        dev_host,
        slurmweb_cluster_name(GATEWAY_PREFERRED_INFRASTRUCTURE),
        GATEWAY_PREFERRED_INFRASTRUCTURE,
        dev_tmp_dir,
    )

    for infrastructure in db.infrastructures.keys():
        # Load agent configuration
        settings = load_settings(
            "conf/vendor/agent.yml", dev_tmp_dir, f"agent-{infrastructure}.ini"
        )
        auth = SlurmrestdAuthentifier(
            settings.slurmrestd.auth,
            settings.slurmrestd.jwt_mode,
            settings.slurmrestd.jwt_user,
            settings.slurmrestd.jwt_key,
            settings.slurmrestd.jwt_lifespan,
            settings.slurmrestd.jwt_token,
        )
        cluster = DevelopmentHostCluster(
            dev_host, infrastructure, settings.slurmrestd.uri, auth
        )

        # Initialize crawlers
        slurmrestd = SlurmrestdCrawler(cluster, auth)
        slurmrestd.crawl_all_assets(cluster)

        gateway = None
        if infrastructure == GATEWAY_PREFERRED_INFRASTRUCTURE:
            gateway = GatewayCrawler(
                token,
                slurmweb_cluster_name(infrastructure),
                infrastructure,
                dev_tmp_dir,
                cluster_obj=cluster,
            )

        if gateway:
            gateway.crawl_all_assets(cluster)

        # Agent and Prometheus (coarse skip if directory has files)
        crawl_metrics = infrastructure == METRICS_PREFERRED_INFRASTRUCTURE

        # Check if agent assets directory exists and has files
        agent_path = ASSETS_ROOT / "agent"
        if agent_path.exists() and any(agent_path.glob("*.json")):
            logger.info("Skipping agent: assets directory already populated")
        else:
            logger.info("Crawling agent")
            cluster.reset_minimal()
            crawl_agent(settings.service.port, token, metrics=crawl_metrics)

        # Check if prometheus assets directory exists and has files
        if crawl_metrics:
            prometheus_path = ASSETS_ROOT / "prometheus"
            if prometheus_path.exists() and any(prometheus_path.glob("*.json")):
                logger.info("Skipping prometheus: assets directory already populated")
            else:
                logger.info("Crawling prometheus")
                crawl_prometheus(settings.metrics.host.geturl(), settings.metrics.job)


if __name__ == "__main__":
    main()
