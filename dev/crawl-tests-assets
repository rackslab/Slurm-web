#!/usr/bin/env python3
#
# Copyright (c) 2024 Rackslab
#
# This file is part of Slurm-web.
#
# SPDX-License-Identifier: GPL-3.0-or-later

from pathlib import Path
import sys
import getpass
import random
import logging


from rfl.log import setup_logger

from crawler.lib import (
    load_settings,
    DevelopmentHostClient,
    DevelopmentHostConnectionError,
    DevelopmentHostCluster,
)
from crawler.slurmrestd import SlurmrestdCrawler
from crawler.agent import crawl_agent
from crawler.gateway import slurmweb_token, GatewayCrawler
from crawler.prometheus import crawl_prometheus

from racksdb import RacksDB
from slurmweb.slurmrestd.auth import SlurmrestdAuthentifier

logger = logging.getLogger("crawl-tests-assets")

DEBUG_FLAGS = ["slurmweb", "rfl", "werkzeug", "urllib3", "crawler"]
DEV_HOST = "firehpc.dev.rackslab.io"
USER = getpass.getuser()
GATEWAY_PREFERRED_INFRASTRUCTURE = "tiny"
METRICS_PREFERRED_INFRASTRUCTURE = "emulator"
# Map between infrastructure names and cluster names that are visible in Slurm-web.
MAP_CLUSTER_NAMES = {"emulator": "atlas"}


def slurmweb_cluster_name(infrastructure: str):
    return MAP_CLUSTER_NAMES.get(infrastructure, infrastructure)


def main() -> None:
    """Crawl and save test assets from Slurm-web gateway, agent and slurmrestd."""

    # Setup logger
    setup_logger(
        debug=True,
        log_flags=["ALL"],
        debug_flags=DEBUG_FLAGS,
    )

    # Search for slurm-web development environment temporary directory
    dev_tmp_dirs = list(Path("/tmp").glob("slurm-web-*"))
    try:
        assert len(dev_tmp_dirs) == 1
    except AssertionError:
        logger.error(
            "Unexpectedly found %d Slurm-web development temporary directories",
            len(dev_tmp_dirs),
        )
        sys.exit(1)
    dev_tmp_dir = dev_tmp_dirs[0]
    logger.info(
        "Slurm-web development environment temporary directory: %s", dev_tmp_dir
    )

    # Load cluster list from RacksDB database
    db = RacksDB.load(db="dev/firehpc/db", schema="../RacksDB/schemas/racksdb.yml")
    logger.info("List of clusters: %s", db.infrastructures.keys())

    dev_host = DevelopmentHostClient(DEV_HOST, USER)
    try:
        dev_host.connect()
    except DevelopmentHostConnectionError as err:
        logger.error(err)
        sys.exit(1)

    # Get Slurm-web JWT for authentication on gateway and agent
    token = slurmweb_token(
        dev_host,
        slurmweb_cluster_name(GATEWAY_PREFERRED_INFRASTRUCTURE),
        GATEWAY_PREFERRED_INFRASTRUCTURE,
        dev_tmp_dir,
    )

    for infrastructure in db.infrastructures.keys():
        # Load agent configuration
        settings = load_settings(
            "conf/vendor/agent.yml", dev_tmp_dir, f"agent-{infrastructure}.ini"
        )
        auth = SlurmrestdAuthentifier(
            settings.slurmrestd.auth,
            settings.slurmrestd.jwt_mode,
            settings.slurmrestd.jwt_user,
            settings.slurmrestd.jwt_key,
            settings.slurmrestd.jwt_lifespan,
            settings.slurmrestd.jwt_token,
        )
        cluster = DevelopmentHostCluster(
            dev_host, infrastructure, settings.slurmrestd.uri, auth
        )

        # Cancel all jobs
        cluster.cancel_all()

        # Resume all nodes
        cluster.nodes_resume()

        # FIXME: check all nodes are OK

        # Move some nodes from production
        nodes = cluster.nodes_partition(cluster.partitions()[0])
        node_down = random.choice(nodes)
        node_drain = random.choice(nodes)
        cluster.node_update(node_down, "DOWN", "CPU dead")
        cluster.node_update(node_drain, "DRAIN", "ECC memory error")

        # Check has no jobs in queue or skip it
        # FIXME: wait cluster to become idle
        if cluster.has_jobs():
            logger.error(
                "Unable to crawl cluster %s with jobs in queue", infrastructure
            )
            continue

        # Submit completed job
        cluster.submit(
            cluster.pick_user(),
            [
                "--ntasks",
                str(1),
            ],
            duration=1,
            wait_running=False,
        )
        # Submit failed job
        cluster.submit(
            cluster.pick_user(),
            [
                "--ntasks",
                str(1),
            ],
            duration=1,
            wait_running=False,
            success=False,
        )
        # Submit timeout job
        cluster.submit(
            cluster.pick_user(),
            [
                "--ntasks",
                str(1),
            ],
            duration=30,
            timelimit=(0, 2),
            wait_running=False,
        )

        # Submit 10 random running jobs
        for idx in range(10):
            cluster.submit(
                cluster.pick_user(),
                [
                    "--ntasks",
                    str(random.choice([1, 4, 16, 32, 64, 128])),
                ],
                duration=random.choice([30, 60, 90]),
                wait_running=False,
            )
        # Submit pending job
        cluster.submit(
            cluster.pick_user(),
            [
                "--ntasks",
                str(random.choice([1, 4, 16, 32, 64, 128])),
                "--begin",
                "now+1hour",
            ],
            duration=30,
            wait_running=False,
        )

        # Crawl assets from components
        if infrastructure == GATEWAY_PREFERRED_INFRASTRUCTURE:
            gateway = GatewayCrawler(
                token,
                slurmweb_cluster_name(infrastructure),
                infrastructure,
                dev_tmp_dir,
            )
            gateway.crawl(
                [
                    "clusters",
                    "users",
                    "login",
                    "stats",
                    "jobs",
                    "nodes",
                    "partitions",
                    "qos",
                    "reservations",
                    "accounts",
                    "racksdb",
                    "metrics",
                ]
            )
        crawl_metrics = infrastructure == METRICS_PREFERRED_INFRASTRUCTURE

        # Crawl agent
        crawl_agent(settings.service.port, token, metrics=crawl_metrics)

        # Crawl prometheus
        if crawl_metrics:
            crawl_prometheus(settings.metrics.host.geturl(), settings.metrics.job)

        slurmrestd = SlurmrestdCrawler(cluster, auth)
        slurmrestd.crawl(
            [
                "ping",
                "errors",
                "jobs",
                "nodes",
                "jobs-gpus",
                "nodes-gpus",
                "partitions",
                "qos",
                "accounts",
                "reservations",
            ]
        )

        # Resume node states
        cluster.node_resume(node_down)
        cluster.node_resume(node_drain)

        # Cancel all jobs
        cluster.cancel_all()

        # If cluster has gpu, launch individual jobs


if __name__ == "__main__":
    main()
